# -*- coding: utf-8 -*-
"""Final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pxrrIsHi_Wl-D73--NcyzdG172-WtWFM
"""

#Library importation

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

#CSV file upload

dataset1 = pd.read_csv('/content/drive/MyDrive/278k_labelled_uri.csv')
dataset1

dataset1.head()

#Dataset columns
dataset1.columns

dataset1.info()

dataset1.shape

## checking NaNs/ missing values
dataset1.isnull().sum()

# Convert labels column from int
dataset1['labels'] = dataset1['labels'].astype(float)

#Encoding uri column
dataset1['uri'],_=pd.factorize(dataset1['uri'])

#Dropping unwanted columns
dataset1.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, inplace=True)

dataset1

"""**EXPLORATORY DATA ANALYSIS**

Visualization
"""

# Visualize the distribution of different emotions(labels) in the dataset using a bar plot.
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='labels', data=dataset1)
plt.title('Distribution of Emotions')
plt.show()

# Explore how individual features are distributed across different emotions(labels)
import seaborn as sns
import matplotlib.pyplot as plt

features = ['duration (ms)', 'danceability', 'energy',
       'loudness', 'speechiness', 'acousticness', 'instrumentalness',
       'liveness', 'valence', 'tempo', 'spec_rate', 'uri']
for feature in features:
    sns.boxplot(x='labels', y=feature, data=dataset1)
    plt.title(f'Distribution of {feature} by Emotion')
    plt.show()

# Examine the correlation between features and emotions(labels)

import seaborn as sns
import matplotlib.pyplot as plt
corr_matrix = dataset1[['labels', 'duration (ms)', 'danceability', 'energy',
       'loudness', 'speechiness', 'acousticness', 'instrumentalness',
       'liveness', 'valence', 'tempo', 'spec_rate', 'uri']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

# Scaling dataset
from sklearn.preprocessing import StandardScaler

# Extract the features to be scaled (exclude non-numeric columns like 'uri' and 'labels')
numeric_features = ['duration (ms)', 'danceability', 'energy',
                    'loudness', 'speechiness', 'acousticness', 'instrumentalness',
                    'liveness', 'valence', 'tempo']

# Extract the non-numeric columns
non_numeric_columns = ['uri', 'labels']

# Separate the dataset into numeric and non-numeric columns
numeric_data = dataset1[numeric_features]
non_numeric_data = dataset1[non_numeric_columns]

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the numeric data
scaled_numeric_data = scaler.fit_transform(numeric_data)

# Create a DataFrame with the scaled numeric data
scaled_numeric_df = pd.DataFrame(scaled_numeric_data, columns=numeric_features)

# Concatenate the scaled numeric data with the non-numeric data
scaled_dataset = pd.concat([non_numeric_data, scaled_numeric_df], axis=1)

# Display the scaled dataset
print(scaled_dataset.head())

"""**Feature importance**"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Extract the features and target variable (X and y)
X = scaled_dataset.drop(['labels'], axis=1)  # Exclude non-feature columns

y = scaled_dataset['labels']

# Split the dataset into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.8, random_state=42)


# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training set
rf_classifier.fit(X_train, y_train)

# Get feature importances from the trained model
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame with feature names and their importance scores
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Print or visualize the feature importances
print(feature_importance_df)

# Plotting the feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Random Forest Feature Importances')
plt.show()

"""# Feature importance by setting a threshold"""

# Set a threshold for feature importance
threshold = 0.015 # Adjust the threshold as needed

# Filter features based on the threshold
selected_features = feature_importance_df[feature_importance_df['Importance'] > threshold]['Feature']

# Print or visualize the selected features
print("Selected Features:")
print(selected_features)

# Subset the training and testing sets with selected features
X_train_selected = X_train[selected_features]
#X_test_selected = X_test[selected_features]


X_test_selected = X_temp[selected_features]

# Initialize the Random Forest classifier with the selected features
rf_classifier_selected = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training set with selected features
rf_classifier_selected.fit(X_train_selected, y_train)

# Evaluate the model on the testing set with selected features
#accuracy_selected_features = rf_classifier_selected.score(X_test_selected, y_test)


accuracy_selected_features = rf_classifier_selected.score(X_test_selected, y_temp)
print(f"Accuracy with selected features: {accuracy_selected_features}")

# Plotting the feature importances for selected features
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df[feature_importance_df['Feature'].isin(selected_features)])
plt.title('Random Forest Feature Importances (Selected Features)')
plt.show()

y

# converting uri integer column to float
X['uri'] = X['uri'].astype(float)

x = selected_features
x

y.shape

"""**TRAINING**

# Convolutional Neural Network (CNN)
"""

import numpy as np
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from keras.utils import to_categorical
from keras.optimizers import Adam


# Assuming X_train_selected and y_train are Pandas DataFrames
X_train_array = X_train_selected.values
y_train_array = y_train.values

# Check the shapes
print(X_train_array.shape, y_train_array.shape)

# Check for missing values
print(np.isnan(X_train_array).any(), np.isnan(y_train_array).any())

# Handle missing values if needed
X_train_array = np.nan_to_num(X_train_array)  # Replace NaN with zero, adjust as needed

# Reshape the input data
X_train_array = X_train_array.reshape(X_train_array.shape[0], X_train_array.shape[1], 1)


input_shape = (11, 1)
num_classes = len(np.unique(y_train_array))

model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_array, y_train_array, epochs=10, validation_data=(X_train_array, y_train_array))
#model.fit(X_train_array, y_train_array, epochs=10, validation_data=(X_val, y_val))

!pip install --upgrade keras scikit-learn

"""**Long Short-Term Memory (LSTM)**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# Assuming X_train and X_val are pandas DataFrames
X_train_array = X_train.values
X_val_array = X_val.values

# Reshape X_train to 3D array with sequences of length 11
sequence_length = 11
X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], sequence_length, 1))

# Convert labels to one-hot encoded format
label_binarizer = LabelBinarizer()
y_train_one_hot = label_binarizer.fit_transform(y_train)

# Assuming X_train_array is your input data
input_shape = X_train_reshaped.shape[1:]

# Define the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=input_shape))
model.add(Dense(4, activation='softmax'))  # Adjust the number of units to match the number of classes
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_reshaped, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2)

# Reshape X_val to 3D array with sequences of length 11
X_val_reshaped = X_val_array.reshape((X_val_array.shape[0], sequence_length, 1))


# Convert y_val to one-hot encoded format if it's not already in that form
y_val_one_hot = label_binarizer.transform(y_val)

# Evaluate the model on the validation set
val_loss, val_accuracy = model.evaluate(X_val_reshaped, y_val_one_hot)
print(f"Validation Accuracy: {val_accuracy}")

# Reshape X_test to 3D array with sequences of length 11
X_test_reshaped = X_test_array.reshape((X_test_array.shape[0], sequence_length, 1))

# Get predictions on the validation set
y_pred = model.predict(X_test_reshaped)

# Print the first few predictions
print("Predictions:")
print(y_pred[:5])

X_test_reshaped.shape

# Convert labels to one-hot encoded format
label_binarizer = LabelBinarizer()
y_temp_one_hot = label_binarizer.fit_transform(y_temp)

# Evaluate the model on the validation set
val_loss, val_accuracy = model.evaluate(X_test_reshaped, y_temp_one_hot)

# Print the results
print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")

"""**Deployment**

"""

#saving model
import pickle
model = model
with open("EmotionPredSong.pkl", "wb") as file:
    pickle.dump(model, file)
with open('scaler.pkl', 'wb') as scaler_file:
  pickle.dump(scaler, scaler_file)